{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639f670b",
   "metadata": {},
   "source": [
    "# ðŸ’• Flan-T5 Romantic Chat Training Pipeline\n",
    "\n",
    "This notebook contains a complete step-by-step guide to fine-tune Google's Flan-T5 model on romantic chat conversations.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "1. **Environment Setup & Dependencies**\n",
    "2. **Data Loading & Preprocessing** \n",
    "3. **Dataset Preparation**\n",
    "4. **Model Configuration**\n",
    "5. **Training Setup**\n",
    "6. **Training Execution**\n",
    "7. **Model Evaluation**\n",
    "8. **Model Saving & Loading**\n",
    "9. **Inference Testing**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1b379",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies ðŸ“¦\n",
    "\n",
    "First, let's install all the necessary libraries for training Flan-T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2028382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "  Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting accelerate\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting evaluate\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25l  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25done\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Collecting absl-py (from rouge_score)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "  Downloading aiohttp-3.12.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/10.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/68.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp312-cp312-macosx_12_0_arm64.whl (30.8 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/30.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pyarrow-20.0.0-cp312-cp312-macosx_12_0_arm64.whl (30.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/11.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.6-cp312-cp312-macosx_11_0_arm64.whl (461 kB)\n",
      "Downloading aiohttp-3.12.6-cp312-cp312-macosx_11_0_arm64.whl (461 kB)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/536.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-macosx_11_0_arm64.whl (121 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-macosx_11_0_arm64.whl (121 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-macosx_11_0_arm64.whl (95 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-macosx_11_0_arm64.whl (95 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25lBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=9c45730ea2ff09ed3ae219575d825d0048d86d0a9e129791edcd74713d807ea2\n",
      "  Stored in directory: /Users/shaswatraj/Library/Caches/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=9c45730ea2ff09ed3ae219575d825d0048d86d0a9e129791edcd74713d807ea2\n",
      "  Stored in directory: /Users/shaswatraj/Library/Caches/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, xxhash, tzdata, tqdm, sympy, safetensors, regex, pyarrow, propcache, numpy, networkx, multidict, joblib, hf-xet, fsspec, frozenlist, filelock, dill, click, aiohappyeyeballs, absl-py, yarl, torch, pandas, nltk, multiprocess, huggingface-hub, aiosignal, tokenizers, rouge_score, aiohttp, accelerate, transformers, datasets, evaluate\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, xxhash, tzdata, tqdm, sympy, safetensors, regex, pyarrow, propcache, numpy, networkx, multidict, joblib, hf-xet, fsspec, frozenlist, filelock, dill, click, aiohappyeyeballs, absl-py, yarl, torch, pandas, nltk, multiprocess, huggingface-hub, aiosignal, tokenizers, rouge_score, aiohttp, accelerate, transformers, datasets, evaluate\n",
      "Successfully installed absl-py-2.3.0 accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.6 aiosignal-1.3.2 click-8.2.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 hf-xet-1.1.2 huggingface-hub-0.32.3 joblib-1.5.1 mpmath-1.3.0 multidict-6.4.4 multiprocess-0.70.16 networkx-3.5 nltk-3.9.1 numpy-2.2.6 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 regex-2024.11.6 rouge_score-0.1.2 safetensors-0.5.3 sentencepiece-0.2.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.52.4 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "Successfully installed absl-py-2.3.0 accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.6 aiosignal-1.3.2 click-8.2.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 hf-xet-1.1.2 huggingface-hub-0.32.3 joblib-1.5.1 mpmath-1.3.0 multidict-6.4.4 multiprocess-0.70.16 networkx-3.5 nltk-3.9.1 numpy-2.2.6 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 regex-2024.11.6 rouge_score-0.1.2 safetensors-0.5.3 sentencepiece-0.2.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.52.4 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (0.32.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (0.32.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch accelerate sentencepiece evaluate rouge_score\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "# For better training performance (optional)\n",
    "!pip install deepspeed  # Optional: for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c2ea8",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries ðŸ“š\n",
    "\n",
    "Import all necessary libraries for data processing, model training, and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34100d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9283b4",
   "metadata": {},
   "source": [
    "## Step 3: Load and Explore Dataset ðŸ”\n",
    "\n",
    "Let's load all the romantic chat data from the LOVE folder and explore its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chat_data(folder_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load all JSON chat data from a folder\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing JSON files\n",
    "    \n",
    "    Returns:\n",
    "        List of chat conversation dictionaries\n",
    "    \"\"\"\n",
    "    all_conversations = []\n",
    "    \n",
    "    # Find all JSON files in the folder\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files:\")\n",
    "    for file_path in json_files:\n",
    "        print(f\"  - {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_conversations.extend(data)\n",
    "                print(f\"    Loaded {len(data)} conversations\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return all_conversations\n",
    "\n",
    "# Load the romantic chat data\n",
    "love_folder = \"/Users/shaswatraj/Desktop/AI/LOVE\"\n",
    "conversations = load_chat_data(love_folder)\n",
    "\n",
    "print(f\"\\nðŸ“Š Total conversations loaded: {len(conversations)}\")\n",
    "print(f\"\\nðŸ“‹ Sample conversation:\")\n",
    "print(json.dumps(conversations[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c99447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "def explore_dataset(conversations: List[Dict]):\n",
    "    \"\"\"\n",
    "    Explore and analyze the chat dataset\n",
    "    \"\"\"\n",
    "    print(\"ðŸ” Dataset Analysis:\")\n",
    "    print(f\"Total conversations: {len(conversations)}\")\n",
    "    \n",
    "    # Analyze input and response lengths\n",
    "    input_lengths = []\n",
    "    response_lengths = []\n",
    "    \n",
    "    for conv in conversations:\n",
    "        if 'input' in conv and 'response' in conv:\n",
    "            input_lengths.append(len(conv['input']))\n",
    "            response_lengths.append(len(conv['response']))\n",
    "    \n",
    "    print(f\"\\nðŸ“ Text Length Statistics:\")\n",
    "    print(f\"Input - Mean: {np.mean(input_lengths):.1f}, Max: {max(input_lengths)}, Min: {min(input_lengths)}\")\n",
    "    print(f\"Response - Mean: {np.mean(response_lengths):.1f}, Max: {max(response_lengths)}, Min: {min(response_lengths)}\")\n",
    "    \n",
    "    # Language analysis (basic)\n",
    "    hindi_count = 0\n",
    "    english_count = 0\n",
    "    mixed_count = 0\n",
    "    \n",
    "    for conv in conversations:\n",
    "        text = conv.get('input', '') + ' ' + conv.get('response', '')\n",
    "        # Simple heuristic for language detection\n",
    "        if any(ord(char) > 127 for char in text):  # Contains non-ASCII (likely Hindi)\n",
    "            if any(char.isascii() and char.isalpha() for char in text):\n",
    "                mixed_count += 1\n",
    "            else:\n",
    "                hindi_count += 1\n",
    "        else:\n",
    "            english_count += 1\n",
    "    \n",
    "    print(f\"\\nðŸŒ Language Distribution:\")\n",
    "    print(f\"English: {english_count}\")\n",
    "    print(f\"Hindi/Hinglish: {hindi_count}\")\n",
    "    print(f\"Mixed: {mixed_count}\")\n",
    "    \n",
    "    # Show sample conversations\n",
    "    print(f\"\\nðŸ“ Sample Conversations:\")\n",
    "    for i, conv in enumerate(conversations[:3]):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Input: {conv.get('input', 'N/A')}\")\n",
    "        print(f\"Response: {conv.get('response', 'N/A')}\")\n",
    "\n",
    "explore_dataset(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a5880",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing ðŸ› ï¸\n",
    "\n",
    "Prepare the data for Flan-T5 training by formatting it properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_conversations(conversations: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Preprocess conversations for T5 training format\n",
    "    \n",
    "    Args:\n",
    "        conversations: List of conversation dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Processed conversations ready for training\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for conv in conversations:\n",
    "        if 'input' in conv and 'response' in conv:\n",
    "            # Extract human input and AI response\n",
    "            human_input = conv['input']\n",
    "            ai_response = conv['response']\n",
    "            \n",
    "            # Remove \"Human: \" and \"AI: \" prefixes if present\n",
    "            if human_input.startswith(\"Human: \"):\n",
    "                human_input = human_input[7:]\n",
    "            if ai_response.startswith(\"AI: \"):\n",
    "                ai_response = ai_response[4:]\n",
    "            \n",
    "            # Format for T5: input should be a task description + context\n",
    "            # T5 is trained with task prefixes\n",
    "            formatted_input = f\"romantic chat: {human_input.strip()}\"\n",
    "            formatted_output = ai_response.strip()\n",
    "            \n",
    "            processed_data.append({\n",
    "                'input_text': formatted_input,\n",
    "                'target_text': formatted_output\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Preprocess the data\n",
    "processed_conversations = preprocess_conversations(conversations)\n",
    "print(f\"âœ… Processed {len(processed_conversations)} conversations\")\n",
    "\n",
    "# Show example of processed data\n",
    "print(f\"\\nðŸ“‹ Processed Format Example:\")\n",
    "for i, item in enumerate(processed_conversations[:2]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {item['input_text']}\")\n",
    "    print(f\"Target: {item['target_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374bb08f",
   "metadata": {},
   "source": [
    "## Step 5: Model and Tokenizer Setup ðŸ¤–\n",
    "\n",
    "Load the Flan-T5 model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/flan-t5-small\"  # Start with small for faster training\n",
    "# Other options: \"google/flan-t5-base\", \"google/flan-t5-large\"\n",
    "\n",
    "print(f\"ðŸ¤– Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"âœ… Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Load model\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "print(f\"âœ… Model loaded and moved to {device}\")\n",
    "print(f\"ðŸ“Š Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_input = \"romantic chat: Hello beautiful, how are you? ðŸ’•\"\n",
    "test_tokens = tokenizer(test_input, return_tensors=\"pt\")\n",
    "print(f\"\\nðŸ§ª Tokenization test:\")\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Tokens: {test_tokens['input_ids'].shape}\")\n",
    "print(f\"Decoded: {tokenizer.decode(test_tokens['input_ids'][0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29011a4",
   "metadata": {},
   "source": [
    "## Step 6: Dataset Preparation ðŸ“Š\n",
    "\n",
    "Split the data and create HuggingFace datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad097ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(processed_conversations: List[Dict], test_size: float = 0.2):\n",
    "    \"\"\"\n",
    "    Create train and validation datasets\n",
    "    \n",
    "    Args:\n",
    "        processed_conversations: List of processed conversation dictionaries\n",
    "        test_size: Fraction of data to use for validation\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict with train and validation splits\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    train_data, val_data = train_test_split(\n",
    "        processed_conversations, \n",
    "        test_size=test_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“Š Dataset split:\")\n",
    "    print(f\"  Training: {len(train_data)} conversations\")\n",
    "    print(f\"  Validation: {len(val_data)} conversations\")\n",
    "    \n",
    "    # Create HuggingFace datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# Create datasets\n",
    "datasets = create_datasets(processed_conversations)\n",
    "print(f\"\\nâœ… Datasets created successfully\")\n",
    "print(f\"Train dataset: {len(datasets['train'])} examples\")\n",
    "print(f\"Validation dataset: {len(datasets['validation'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the input and target texts\n",
    "    \"\"\"\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False  # We'll pad later in the data collator\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"ðŸ”„ Tokenizing datasets...\")\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=datasets['train'].column_names\n",
    ")\n",
    "\n",
    "print(\"âœ… Tokenization complete\")\n",
    "print(f\"Tokenized train dataset: {len(tokenized_datasets['train'])} examples\")\n",
    "print(f\"Tokenized validation dataset: {len(tokenized_datasets['validation'])} examples\")\n",
    "\n",
    "# Show tokenized example\n",
    "print(f\"\\nðŸ“‹ Tokenized Example:\")\n",
    "example = tokenized_datasets['train'][0]\n",
    "print(f\"Input IDs shape: {np.array(example['input_ids']).shape}\")\n",
    "print(f\"Labels shape: {np.array(example['labels']).shape}\")\n",
    "print(f\"Decoded input: {tokenizer.decode(example['input_ids'], skip_special_tokens=True)}\")\n",
    "print(f\"Decoded label: {tokenizer.decode(example['labels'], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deed3c1",
   "metadata": {},
   "source": [
    "## Step 7: Training Configuration âš™ï¸\n",
    "\n",
    "Set up training arguments and data collator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"./flan-t5-romantic-chat\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,  # Start with 3 epochs\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Disable wandb reporting\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 4 * 2 = 8\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    learning_rate=5e-5,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"âš™ï¸ Training Configuration:\")\n",
    "print(f\"  Output directory: {output_dir}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision (FP16): {training_args.fp16}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Data collator configured for dynamic padding\")\n",
    "\n",
    "# Test the data collator\n",
    "test_batch = [tokenized_datasets['train'][i] for i in range(2)]\n",
    "collated_batch = data_collator(test_batch)\n",
    "print(f\"\\nðŸ§ª Data Collator Test:\")\n",
    "print(f\"Batch input_ids shape: {collated_batch['input_ids'].shape}\")\n",
    "print(f\"Batch labels shape: {collated_batch['labels'].shape}\")\n",
    "print(f\"Batch attention_mask shape: {collated_batch['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad5b9d",
   "metadata": {},
   "source": [
    "## Step 8: Trainer Setup ðŸ‹ï¸\n",
    "\n",
    "Create the Trainer with evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e89999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge_metric = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute ROUGE metrics for evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels (used for padding) with tokenizer.pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    # Extract ROUGE scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "        \"rougeLsum\": result[\"rougeLsum\"]\n",
    "    }\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured successfully\")\n",
    "print(f\"ðŸ“Š Training will start with {len(tokenized_datasets['train'])} training examples\")\n",
    "print(f\"ðŸ“Š Evaluation will use {len(tokenized_datasets['validation'])} validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733c9d3",
   "metadata": {},
   "source": [
    "## Step 9: Model Training ðŸš€\n",
    "\n",
    "Start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model before training\n",
    "print(\"ðŸ” Pre-training model check:\")\n",
    "test_input = \"romantic chat: I miss you so much ðŸ’”\"\n",
    "test_encoding = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        test_encoding.input_ids,\n",
    "        max_length=50,\n",
    "        num_beams=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    pre_training_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"Pre-training output: {pre_training_output}\")\n",
    "\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "print(\"This may take a while depending on your hardware.\")\n",
    "print(\"ðŸ“Š Training progress will be shown below:\")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training completed!\")\n",
    "print(f\"ðŸ“Š Training Results:\")\n",
    "print(f\"  Final loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874f236",
   "metadata": {},
   "source": [
    "## Step 10: Model Evaluation ðŸ“ˆ\n",
    "\n",
    "Evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7292d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"ðŸ“Š Evaluating trained model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        metric_name = key.replace('eval_', '')\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# Test the model with sample inputs\n",
    "print(\"\\nðŸ§ª Testing trained model with sample inputs:\")\n",
    "\n",
    "test_cases = [\n",
    "    \"romantic chat: Good morning beautiful! â˜€ï¸\",\n",
    "    \"romantic chat: I'm feeling sad today ðŸ˜¢\",\n",
    "    \"romantic chat: Tumhe pata hai main tumse kitna pyaar karta hun? ðŸ’•\",\n",
    "    \"romantic chat: What would you do if I was there with you?\",\n",
    "    \"romantic chat: Mujhe tumhari yaad aa rahi hai ðŸ’­\"\n",
    "]\n",
    "\n",
    "for test_input in test_cases:\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=100,\n",
    "            num_beams=3,\n",
    "            early_stopping=True,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nðŸ’¬ Input: {test_input.replace('romantic chat: ', '')}\")\n",
    "    print(f\"ðŸ¤– Response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f8d0d",
   "metadata": {},
   "source": [
    "## Step 11: Save the Trained Model ðŸ’¾\n",
    "\n",
    "Save the model and tokenizer for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd125b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "save_directory = \"./flan-t5-romantic-chat-final\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ’¾ Saving model to {save_directory}...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(\"âœ… Model and tokenizer saved successfully!\")\n",
    "print(f\"ðŸ“ Model location: {save_directory}\")\n",
    "\n",
    "# Also save training configuration\n",
    "config_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"training_examples\": len(tokenized_datasets['train']),\n",
    "    \"validation_examples\": len(tokenized_datasets['validation']),\n",
    "    \"epochs\": training_args.num_train_epochs,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"final_eval_loss\": eval_results.get('eval_loss', 'N/A'),\n",
    "    \"final_rouge1\": eval_results.get('eval_rouge1', 'N/A')\n",
    "}\n",
    "\n",
    "with open(f\"{save_directory}/training_info.json\", 'w') as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(\"ðŸ“‹ Training configuration saved to training_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a968f0",
   "metadata": {},
   "source": [
    "## Step 12: Load and Test Saved Model ðŸ”„\n",
    "\n",
    "Demonstrate how to load and use the saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and test the saved model\n",
    "def load_and_test_model(model_path: str):\n",
    "    \"\"\"\n",
    "    Load the saved model and test it\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ Loading model from {model_path}...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    loaded_tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    loaded_model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    loaded_model.to(device)\n",
    "    \n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "    \n",
    "    return loaded_tokenizer, loaded_model\n",
    "\n",
    "def generate_romantic_response(model, tokenizer, user_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a romantic response using the trained model\n",
    "    \"\"\"\n",
    "    # Format input for the model\n",
    "    formatted_input = f\"romantic chat: {user_input}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the loading function\n",
    "loaded_tokenizer, loaded_model = load_and_test_model(save_directory)\n",
    "\n",
    "# Interactive testing\n",
    "print(\"\\nðŸ’• Interactive Romantic Chat Test:\")\n",
    "print(\"Type messages and see how the model responds!\\n\")\n",
    "\n",
    "test_messages = [\n",
    "    \"Hey beautiful, how are you today?\",\n",
    "    \"I'm missing you so much right now\", \n",
    "    \"Tumhe pata hai tum kitne special ho?\",\n",
    "    \"What would you do if I surprised you?\",\n",
    "    \"I love talking to you\"\n",
    "]\n",
    "\n",
    "for message in test_messages:\n",
    "    response = generate_romantic_response(loaded_model, loaded_tokenizer, message)\n",
    "    print(f\"ðŸ’¬ You: {message}\")\n",
    "    print(f\"ðŸ¤– AI: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813c200",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### ðŸ“Š Summary\n",
    "You have successfully fine-tuned a Flan-T5 model on romantic chat conversations! The model can now:\n",
    "\n",
    "- Generate romantic and flirty responses\n",
    "- Handle both English and Hindi/Hinglish inputs\n",
    "- Maintain conversational context and emotional tone\n",
    "- Respond with appropriate emojis and emotional expressions\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. **Improve the Model:**\n",
    "   - Add more diverse training data\n",
    "   - Experiment with different model sizes (base, large)\n",
    "   - Fine-tune hyperparameters\n",
    "   - Add more evaluation metrics\n",
    "\n",
    "2. **Deploy the Model:**\n",
    "   - Create a chatbot interface\n",
    "   - Build a web application\n",
    "   - Deploy to cloud platforms\n",
    "   - Create mobile app integration\n",
    "\n",
    "3. **Advanced Features:**\n",
    "   - Add personality consistency\n",
    "   - Implement conversation memory\n",
    "   - Add safety filters\n",
    "   - Create different conversation modes\n",
    "\n",
    "### ðŸ“ Files Generated\n",
    "- `./flan-t5-romantic-chat-final/` - Trained model and tokenizer\n",
    "- `./flan-t5-romantic-chat/` - Training checkpoints and logs\n",
    "- `training_info.json` - Training configuration and results\n",
    "\n",
    "---\n",
    "**Happy chatting! ðŸ’•**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375eb3a",
   "metadata": {},
   "source": [
    "## Bonus: Simple Chat Interface ðŸ’¬\n",
    "\n",
    "Create a simple interactive chat interface to test your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Simple interactive chat interface\n",
    "    \"\"\"\n",
    "    print(\"ðŸ’• Welcome to your Romantic AI Chatbot!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"ðŸ’¬ You: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"ðŸ¤– AI: Goodbye my love! ðŸ’•\")\n",
    "            break\n",
    "        \n",
    "        if user_input.strip():\n",
    "            response = generate_romantic_response(loaded_model, loaded_tokenizer, user_input)\n",
    "            print(f\"ðŸ¤– AI: {response}\\n\")\n",
    "        else:\n",
    "            print(\"ðŸ¤– AI: Please say something, darling! ðŸ’–\\n\")\n",
    "\n",
    "# Uncomment the line below to start the interactive chat\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7fc7e",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Training Tips & Best Practices\n",
    "\n",
    "### Performance Optimization\n",
    "- **GPU Memory**: If you run out of GPU memory, reduce `per_device_train_batch_size`\n",
    "- **Training Speed**: Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- **Model Size**: Start with `flan-t5-small`, then try `flan-t5-base` or `flan-t5-large`\n",
    "\n",
    "### Data Quality\n",
    "- **Diverse Examples**: Include various conversation styles and emotions\n",
    "- **Language Mix**: Balance English and Hindi/Hinglish examples\n",
    "- **Quality Control**: Remove inappropriate or low-quality conversations\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- **Learning Rate**: Try values between 1e-5 and 1e-4\n",
    "- **Epochs**: Monitor validation loss to avoid overfitting\n",
    "- **Temperature**: Higher values (0.8-1.2) for more creative responses\n",
    "\n",
    "### Evaluation\n",
    "- **Human Evaluation**: Have humans rate response quality\n",
    "- **Conversation Flow**: Test multi-turn conversations\n",
    "- **Safety**: Check for inappropriate content generation\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provides a complete pipeline for training romantic chatbots using Flan-T5. Customize the parameters and data according to your specific needs!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c11950",
   "metadata": {},
   "source": [
    "## ðŸ“ Model Usage Notes\n",
    "\n",
    "### Loading the Model in Production\n",
    "\n",
    "```python\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"./flan-t5-romantic-chat-final\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Generate response\n",
    "def chat_response(user_input):\n",
    "    input_text = f\"romantic chat: {user_input}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=100, temperature=0.7, do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "```\n",
    "\n",
    "### âš ï¸ Important Considerations\n",
    "\n",
    "1. **Content Safety**: Always implement content filters for production use\n",
    "2. **User Privacy**: Don't store personal conversations without consent\n",
    "3. **Bias Monitoring**: Regularly check for and mitigate potential biases\n",
    "4. **Performance**: Monitor response quality and retrain as needed\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ Ready to create amazing romantic conversations! Your AI companion is now trained and ready to spread love! ðŸ’•**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
